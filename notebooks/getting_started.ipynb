{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Lab - Getting Started Tutorial\n",
    "\n",
    "Welcome to the Artificial Intelligence Lab! This notebook demonstrates how to use the implemented algorithms.\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "- How to generate sample datasets\n",
    "- How to train machine learning models from scratch\n",
    "- How to evaluate model performance\n",
    "- How to visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from datasets.sample_data import (\n",
    "    generate_regression_data, \n",
    "    generate_classification_data, \n",
    "    generate_clustering_data\n",
    ")\n",
    "from machine_learning.linear_regression import LinearRegression\n",
    "from machine_learning.kmeans import KMeans\n",
    "from neural_networks.perceptron import Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 1. Linear Regression Example\n",
    "\n",
    "Let's start with a simple regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample regression data\n",
    "X_reg, y_reg = generate_regression_data(n_samples=100, n_features=1, noise=0.2)\n",
    "\n",
    "# Create and train the model\n",
    "lr_model = LinearRegression(learning_rate=0.01, n_iterations=1000)\n",
    "lr_model.fit(X_reg, y_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lr_model.predict(X_reg)\n",
    "\n",
    "# Calculate R¬≤ score\n",
    "r2 = lr_model.score(X_reg, y_reg)\n",
    "print(f\"R¬≤ Score: {r2:.4f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_reg[:, 0], y_reg, alpha=0.6, label='Actual')\n",
    "plt.plot(X_reg[:, 0], y_pred, 'r-', label='Predicted')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title(f'Linear Regression (R¬≤ = {r2:.4f})')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(lr_model.cost_history)\n",
    "plt.title('Cost Function Over Iterations')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ 2. Classification with Perceptron\n",
    "\n",
    "Now let's try binary classification using a Perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification data\n",
    "X_clf, y_clf = generate_classification_data(n_samples=100, n_features=2, n_classes=2)\n",
    "y_clf = np.where(y_clf == 0, -1, 1)  # Convert to -1, 1 labels\n",
    "\n",
    "# Create and train perceptron\n",
    "perceptron = Perceptron(learning_rate=0.1, n_iterations=1000)\n",
    "perceptron.fit(X_clf, y_clf)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = perceptron.score(X_clf, y_clf)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Converged after {len(perceptron.errors)} iterations\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Decision boundary\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_clf[y_clf == 1, 0], X_clf[y_clf == 1, 1], c='red', marker='o', label='Class 1', alpha=0.7)\n",
    "plt.scatter(X_clf[y_clf == -1, 0], X_clf[y_clf == -1, 1], c='blue', marker='s', label='Class -1', alpha=0.7)\n",
    "\n",
    "# Plot decision boundary\n",
    "x_min, x_max = X_clf[:, 0].min() - 1, X_clf[:, 0].max() + 1\n",
    "if abs(perceptron.weights[1]) > 1e-6:\n",
    "    x1_line = np.array([x_min, x_max])\n",
    "    x2_line = (-perceptron.weights[0] * x1_line - perceptron.bias) / perceptron.weights[1]\n",
    "    plt.plot(x1_line, x2_line, 'k-', linewidth=2, label='Decision Boundary')\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title(f'Perceptron Classification (Accuracy = {accuracy:.4f})')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Learning curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(perceptron.errors) + 1), perceptron.errors, marker='o')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Number of Errors')\n",
    "plt.title('Perceptron Learning Curve')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç 3. K-Means Clustering\n",
    "\n",
    "Finally, let's explore unsupervised learning with K-Means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate clustering data\n",
    "X_cluster, true_labels = generate_clustering_data(n_samples=300, centers=3)\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(k=3, max_iters=100, random_state=42)\n",
    "kmeans.fit(X_cluster)\n",
    "predicted_labels = kmeans.predict(X_cluster)\n",
    "\n",
    "# Calculate inertia\n",
    "inertia = kmeans.inertia(X_cluster)\n",
    "print(f\"Inertia: {inertia:.2f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Original data with true labels\n",
    "plt.subplot(1, 3, 1)\n",
    "scatter1 = plt.scatter(X_cluster[:, 0], X_cluster[:, 1], c=true_labels, alpha=0.6)\n",
    "plt.title('True Clusters')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True)\n",
    "plt.colorbar(scatter1)\n",
    "\n",
    "# K-means results\n",
    "plt.subplot(1, 3, 2)\n",
    "scatter2 = plt.scatter(X_cluster[:, 0], X_cluster[:, 1], c=predicted_labels, alpha=0.6)\n",
    "plt.scatter(kmeans.centroids[:, 0], kmeans.centroids[:, 1], \n",
    "           c='black', marker='x', s=100, linewidths=3, label='Centroids')\n",
    "plt.title(f'K-Means Results (Inertia = {inertia:.1f})')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.colorbar(scatter2)\n",
    "\n",
    "# Analyze different k values\n",
    "plt.subplot(1, 3, 3)\n",
    "k_values = range(1, 8)\n",
    "inertias = []\n",
    "\n",
    "for k in k_values:\n",
    "    km = KMeans(k=k, max_iters=100, random_state=42)\n",
    "    km.fit(X_cluster)\n",
    "    inertias.append(km.inertia(X_cluster))\n",
    "\n",
    "plt.plot(k_values, inertias, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Conclusion\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "\n",
    "1. **Implemented Linear Regression** - Used gradient descent to find the best fit line\n",
    "2. **Built a Perceptron** - Created a binary classifier from scratch\n",
    "3. **Applied K-Means Clustering** - Discovered hidden patterns in unlabeled data\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "- Explore other algorithms in the repository\n",
    "- Try different datasets and parameters\n",
    "- Implement your own variations\n",
    "- Compare with scikit-learn implementations\n",
    "\n",
    "Happy learning! ü§ñ‚ú®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}